{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a307d2d7",
   "metadata": {},
   "source": [
    "# Polynomial Regression with Interaction Terms  \n",
    "## Modeling Stellar Luminosity Using Mass and Temperature\n",
    "- Student: Juan José Mejía Celis\n",
    "- Class: AREP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e06d36",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84caeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7813a2d",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa380eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.array([0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4])\n",
    "T = np.array([3800, 4400, 5800, 6400, 6900, 7400, 7900, 8300, 8800, 9200])\n",
    "L = np.array([0.15, 0.35, 1.00, 2.30, 4.10, 7.00, 11.2, 17.5, 25.0, 35.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea61713",
   "metadata": {},
   "source": [
    "## Dataset Visualization with Temperature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58edf452",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(M, L, c=T, cmap=\"viridis\")\n",
    "plt.xlabel(\"Stellar Mass (M⊙)\")\n",
    "plt.ylabel(\"Luminosity (L⊙)\")\n",
    "plt.colorbar(label=\"Temperature (K)\")\n",
    "plt.title(\"Luminosity vs Mass with Temperature Encoding\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0b8f8a",
   "metadata": {},
   "source": [
    "At fixed mass, hotter stars tend to be more luminous, suggesting the importance\n",
    "of temperature and interaction effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0928d4",
   "metadata": {},
   "source": [
    "## Polynomial Feature Engineering and Design Matrix Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2b4284",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.column_stack([\n",
    "    M,\n",
    "    T,\n",
    "    M**2,\n",
    "    M * T\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227c449c",
   "metadata": {},
   "source": [
    "## Linear Model Definition and Mean Squared Error Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c61ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b):\n",
    "    return X @ w + b\n",
    "\n",
    "def mse(actual, predicted):\n",
    "    return np.mean((actual - predicted)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e82c0eb",
   "metadata": {},
   "source": [
    "## Vectorized Gradient Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bf9975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(X, y, w, b):\n",
    "    N = len(y)\n",
    "    pred = predict(X, w, b)\n",
    "    error = pred - y\n",
    "    grad_w = (2/N) * X.T @ error\n",
    "    grad_b = (2/N) * np.sum(error)\n",
    "    return grad_w, grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10820239",
   "metadata": {},
   "source": [
    "## Gradient Descent Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f1c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, alpha=1e-12, iterations=5000):\n",
    "    w = np.zeros(X.shape[1])\n",
    "    b = 0.0\n",
    "    loss_hist = []\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        pred = predict(X, w, b)\n",
    "        loss_hist.append(mse(y, pred))\n",
    "\n",
    "        grad_w, grad_b = gradients(X, y, w, b)\n",
    "        w -= alpha * grad_w\n",
    "        b -= alpha * grad_b\n",
    "\n",
    "    return w, b, loss_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f6f794",
   "metadata": {},
   "source": [
    "## Feature Selection Experiment and Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdfb49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"M1: [M, T]\": X[:, :2],\n",
    "    \"M2: [M, T, M^2]\": X[:, :3],\n",
    "    \"M3: [M, T, M^2, M*T]\": X\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, X_model in models.items():\n",
    "    w, b, loss_hist = train(X_model, L)\n",
    "    results[name] = (w, b, loss_hist[-1])\n",
    "    print(f\"{name}: final_loss={loss_hist[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46307558",
   "metadata": {},
   "source": [
    "## Predicted Versus Actual Luminosity Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66125bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, X_model in models.items():\n",
    "    w, b, _ = train(X_model, L)\n",
    "    L_pred = predict(X_model, w, b)\n",
    "\n",
    "    plt.scatter(L, L_pred)\n",
    "    plt.plot([0, 40], [0, 40], \"--\")\n",
    "    plt.xlabel(\"Actual Luminosity\")\n",
    "    plt.ylabel(\"Predicted Luminosity\")\n",
    "    plt.title(name)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79543705",
   "metadata": {},
   "source": [
    "## Cost Sensitivity Analysis for the Interaction Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a7e816",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_full, b_full, _ = train(X, L)\n",
    "\n",
    "w_interaction = np.linspace(w_full[3] - 1e-6, w_full[3] + 1e-6, 100)\n",
    "costs = []\n",
    "\n",
    "for w_mt in w_interaction:\n",
    "    w_temp = w_full.copy()\n",
    "    w_temp[3] = w_mt\n",
    "    costs.append(mse(L, predict(X, w_temp, b_full)))\n",
    "\n",
    "plt.plot(w_interaction, costs)\n",
    "plt.xlabel(\"Interaction Coefficient (M * T)\")\n",
    "plt.ylabel(\"Cost (MSE)\")\n",
    "plt.title(\"Cost vs Interaction Term\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90c1c47",
   "metadata": {},
   "source": [
    "A clear minimum indicates that the interaction term significantly contributes\n",
    "to explaining stellar luminosity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dad631",
   "metadata": {},
   "source": [
    "## Inference on a New Stellar Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e35c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_new = 1.3\n",
    "T_new = 6600\n",
    "\n",
    "X_new = np.array([[M_new, T_new, M_new**2, M_new*T_new]])\n",
    "L_pred = predict(X_new, w_full, b_full)\n",
    "\n",
    "print(f\"Luminosity prediction: {L_pred[0]:.2f} L⊙\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a662aea1",
   "metadata": {},
   "source": [
    "The predicted luminosity lies between neighboring data points and is physically\n",
    "reasonable for a main-sequence star.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e4f01d",
   "metadata": {},
   "source": [
    "## Conclusions and Model Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3040f5ae",
   "metadata": {},
   "source": [
    "Polynomial and interaction terms substantially improve model expressiveness,\n",
    "capturing nonlinear stellar behavior beyond linear regression.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
